{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c866f5d3",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "Importing necessary libraries for data manipulation (`pandas`, `json`), visualization (`matplotlib`), and language statistics (`wordfreq`). The `wordfreq` library provides the \"ground truth\" frequency of words in different languages, which is essential for our comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cad866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from wordfreq import top_n_list, word_frequency\n",
    "\n",
    "print(\"Libraries loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233d4fcd",
   "metadata": {},
   "source": [
    "## 2. Language Confidence Score\n",
    "Core logic for language detection.\n",
    "Calculating a score based on the intersection between word counts in given text and the top $k$ most frequent words in a target language. The score increases when the text frequently uses words that are common in the target language (e.g., \"the\", \"and\" in English)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b590ac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_confidence_score(word_counts, language_words_with_frequency):\n",
    "    \"\"\"\n",
    "    Calculates a confidence score indicating how well the word_counts match the language profile.\n",
    "    \"\"\"\n",
    "    score = 0.0\n",
    "    total_text_words = sum(word_counts.values())\n",
    "    \n",
    "    if total_text_words == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Normalize text counts to frequencies (0.0 to 1.0)\n",
    "    text_freqs = {w: c / total_text_words for w, c in word_counts.items()}\n",
    "    \n",
    "    # Calculate score based on overlap\n",
    "    for word, lang_freq in language_words_with_frequency.items():\n",
    "        if word in text_freqs:\n",
    "            # We multiply by the frequency in the text to weight it by usage\n",
    "            score += text_freqs[word] * lang_freq\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f79d47d",
   "metadata": {},
   "source": [
    "## 3. Data Loading Utility\n",
    "A simple helper function to safely load JSON files from the disk. It handles `FileNotFoundError` gracefully to ensure the analysis can proceed even if one file is missing or misnamed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fb2697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(filename):\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: {filename} not found.\")\n",
    "        return {}\n",
    "\n",
    "print(\"Loader function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a88150",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae239bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your 5 test files\n",
    "datasets = {\n",
    "    \"Wiki Long (En)\": load_json('data_wiki_long.json'),\n",
    "    \"Wiki Short (En)\": load_json('data_wiki_short.json'),\n",
    "    \"External (En)\":  load_json('data_ext_en.json'),\n",
    "    \"External (Pl)\":  load_json('data_ext_pl.json'),\n",
    "    \"External (De)\":  load_json('data_ext_de.json')\n",
    "}\n",
    "\n",
    "print(\"Datasets loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb90e0ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a1cdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "k_values = [3, 10, 100, 1000]\n",
    "languages = ['en', 'pl', 'de']  # English, Polish, German\n",
    "\n",
    "results = []\n",
    "\n",
    "for k in k_values:\n",
    "    # 1. Prepare Language Data for this K\n",
    "    lang_profiles = {}\n",
    "    for lang in languages:\n",
    "        # Get top K words\n",
    "        top_words = top_n_list(lang, k)\n",
    "        # Get their frequencies\n",
    "        lang_profiles[lang] = {w: word_frequency(w, lang) for w in top_words}\n",
    "        \n",
    "    # 2. Score each dataset against each language\n",
    "    for data_name, word_counts in datasets.items():\n",
    "        for lang in languages:\n",
    "            score = lang_confidence_score(word_counts, lang_profiles[lang])\n",
    "            \n",
    "            results.append({\n",
    "                'k': k,\n",
    "                'Dataset': data_name,\n",
    "                'Language': lang,\n",
    "                'Score': score\n",
    "            })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cea7df6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31522c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot data for plotting\n",
    "# We create a separate chart for each K value\n",
    "for k in k_values:\n",
    "    subset = df_results[df_results['k'] == k]\n",
    "    pivot = subset.pivot(index='Dataset', columns='Language', values='Score')\n",
    "    \n",
    "    pivot.plot(kind='bar', figsize=(10, 6))\n",
    "    plt.title(f'Language Confidence Score (top k={k} words)')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
